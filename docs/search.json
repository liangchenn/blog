[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nHi, I’m Liang-Cheng. Here is where I keep notes for self-taught stuffs."
  },
  {
    "objectID": "posts/hypothesis-testing/hypothesis-testing.html",
    "href": "posts/hypothesis-testing/hypothesis-testing.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A frequentist method to make statistical inference\nConcepts:\n\nwant to know how extreme our observed data is based on the belief\nif the probability of having data more extreme than current data is very small, then we could say the belief may not be correct\n\nBuilding blocks\n\nbelief: null hypothesis\nlevel of extreme: limiting distribution\nprobability of having data more extreme: p-value\n\n\n\n\nNow we could generate random samples to illustrate above concepts.\nFirst use the i.i.d random sample from binomial distribution.\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# binomial setups\np = 0.05\nsize = 1000\n\n# random sample\nsamples = np.random.binomial(n=1, p=p, size=size)\n\nTheoratically, the variance of a binomial distribution will follow:\n\\(Var(X) = p \\cdot (1-p)\\)\n\nprint(f\"Theoratical value: {p * (1-p)}\")\nprint(f\"Empirical   value: {samples.var()}\")\n\nTheoratical value: 0.0475\nEmpirical   value: 0.041150999999999986\n\n\n\n\n\nWe could obtain the limiting distribution mathematically.\nGoes from sampling distribution (the distribution of statistic);\nTo the limiting distribution (the distribution when size goes to infinitely large)\n\n\nFor random sample \\(X_i \\sim (\\mu, \\sigma^2)\\)\nthe sampled mean will be \\(\\bar{X_n} = \\frac{\\sum_i X_i}{n}\\), where \\(n\\) is the size of sample\nThen we will know that:\n\n\\(E[\\bar X_n] = \\frac{1}{n} \\cdot n \\cdot E[X_i] = \\mu\\)\n\\(Var(\\bar X_n) = \\frac{1}{n^2} \\cdot n \\cdot Var(X_i) = \\frac{\\sigma^2}{n}\\)\n\nThe sampling distribution of sampled mean will be \\(\\bar X_n \\sim (\\mu, \\frac{\\sigma^2}{n})\\)\n\n\n\nBy the C.L.T, we know that:\nAs \\(n \\rightarrow \\infty\\), \\(\\bar X_n \\sim^d N(\\mu, \\frac{\\sigma^2}{n})\\)\n\n\n\n\nEmpirically, it is like you repeat the random draw over and over agian, and collect the mean each time. (bootstrapping method)\nThe histogram of the means will be the sampling distribution.\n\n# simulate the random draws for 500 times\n\nn_simulation = 500\nbootstrap_sampled_mean = np.zeros(n_simulation)\n\nfor i in range(n_simulation):\n    \n    bootstrap_samples = np.random.choice(samples, size=len(samples), replace=True)\n    \n    bootstrap_sampled_mean[i] = bootstrap_samples.mean()\n\n\nsns.histplot(bootstrap_sampled_mean, stat='probability')\nplt.title(fr\"Sampling Distribution for $\\bar X_n$, N={len(samples)}\")\nplt.show()\n\n\n\n\nAdd the PDF for the empirical and theoratical distribution\n\nfrom scipy.stats import norm\n\n\nsmean = samples.mean()\nsvar = samples.var() / len(samples)\nsstd = np.sqrt(svar)\n\n\nsns.kdeplot(bootstrap_sampled_mean, label='empirical', color='salmon')\nsns.lineplot(\n    x=(x := np.linspace(min(bootstrap_sampled_mean), max(bootstrap_sampled_mean))), \n    y=norm.pdf(x, loc=smean, scale=sstd),\n    color='skyblue',\n    label='theoratical'\n)\nplt.title(fr\"PDF for $\\bar X_n$, N={len(samples)}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nNow suppose we want to know if the mean is \\(p = 0.075\\)\nWe construct the belief, the null hypothesis, \\(H_0: p = 0.075\\)\nGiven the null hypothesis is true we construct the theoratical distribution for sampled mean.\n\ntrue_mean = 0.065\ntrue_se = np.sqrt(true_mean * (1-true_mean) / size)\n\n\nx = np.linspace(true_mean - 4 * true_se, true_mean + 4 * true_se)\ny = norm.pdf(x, loc=true_mean, scale=true_se)\n\nx_fill = np.linspace(true_mean - 4 * true_se, samples.mean())\ny_fill = norm.pdf(x_fill, loc=true_mean, scale=true_se)\n\n\nsns.lineplot(x=x, y=y, label='theoratical')\nplt.axvline(x=samples.mean(), linestyle='-', color='salmon', label='observed mean')\nplt.axvline(x=true_mean, color='lightgray', label='H0 mean')\nplt.fill_between(x_fill, y_fill, color='skyblue', alpha=0.3, label='extreme prob.')\n\nplt.title(r\"$\\bar X_n$ Sampling Dist. | H0\")\nplt.legend()\nplt.show()\n\n\n\n\nThe shaded area is the probability of observing a realized sample mean more extreme than the current one, which is the p-value.\nThe smaller it gets, the more likely the null hypothesis to be false. Then we could set a conventional threshold (e.g. 5%), then use the threshold to reject the H0.\nWe use conventional 5% threshold. For two-sided test, it will be 2.5% for each side.\nNow we could obtain the probability \\(Pr(X &lt;= observed\\_mean | H0)\\)\n\npvalue = norm.cdf(samples.mean(), loc=true_mean, scale=true_se)\n\n\nprint(f\"The p-value is: {pvalue:.4f}\")\n\nif pvalue &lt;= 0.025:\n    print(f\"Result: Reject H0\")\nelse:\n    print(f\"Result: Could not reject H0\")\n\nThe p-value is: 0.0024\nResult: Reject H0\n\n\n\n\n\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom scipy.stats import ttest_1samp\nfrom scipy.stats.distributions import t\n\n\nttest_1samp(samples, popmean=true_mean, alternative='two-sided')\n\nTtest_1sampResult(statistic=-3.4277980568938475, pvalue=0.000633319473855572)\n\n\n\nproportions_ztest(samples.sum(), len(samples), value=true_mean, alternative='two-sided')\n\n(-3.429513242418691, 0.0006046649260488792)"
  },
  {
    "objectID": "posts/hypothesis-testing/hypothesis-testing.html#data",
    "href": "posts/hypothesis-testing/hypothesis-testing.html#data",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Now we could generate random samples to illustrate above concepts.\nFirst use the i.i.d random sample from binomial distribution.\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# binomial setups\np = 0.05\nsize = 1000\n\n# random sample\nsamples = np.random.binomial(n=1, p=p, size=size)\n\nTheoratically, the variance of a binomial distribution will follow:\n\\(Var(X) = p \\cdot (1-p)\\)\n\nprint(f\"Theoratical value: {p * (1-p)}\")\nprint(f\"Empirical   value: {samples.var()}\")\n\nTheoratical value: 0.0475\nEmpirical   value: 0.041150999999999986"
  },
  {
    "objectID": "posts/hypothesis-testing/hypothesis-testing.html#limiting-distribution",
    "href": "posts/hypothesis-testing/hypothesis-testing.html#limiting-distribution",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We could obtain the limiting distribution mathematically.\nGoes from sampling distribution (the distribution of statistic);\nTo the limiting distribution (the distribution when size goes to infinitely large)\n\n\nFor random sample \\(X_i \\sim (\\mu, \\sigma^2)\\)\nthe sampled mean will be \\(\\bar{X_n} = \\frac{\\sum_i X_i}{n}\\), where \\(n\\) is the size of sample\nThen we will know that:\n\n\\(E[\\bar X_n] = \\frac{1}{n} \\cdot n \\cdot E[X_i] = \\mu\\)\n\\(Var(\\bar X_n) = \\frac{1}{n^2} \\cdot n \\cdot Var(X_i) = \\frac{\\sigma^2}{n}\\)\n\nThe sampling distribution of sampled mean will be \\(\\bar X_n \\sim (\\mu, \\frac{\\sigma^2}{n})\\)\n\n\n\nBy the C.L.T, we know that:\nAs \\(n \\rightarrow \\infty\\), \\(\\bar X_n \\sim^d N(\\mu, \\frac{\\sigma^2}{n})\\)"
  },
  {
    "objectID": "posts/hypothesis-testing/hypothesis-testing.html#how-could-we-understand-it-empirically",
    "href": "posts/hypothesis-testing/hypothesis-testing.html#how-could-we-understand-it-empirically",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Empirically, it is like you repeat the random draw over and over agian, and collect the mean each time. (bootstrapping method)\nThe histogram of the means will be the sampling distribution.\n\n# simulate the random draws for 500 times\n\nn_simulation = 500\nbootstrap_sampled_mean = np.zeros(n_simulation)\n\nfor i in range(n_simulation):\n    \n    bootstrap_samples = np.random.choice(samples, size=len(samples), replace=True)\n    \n    bootstrap_sampled_mean[i] = bootstrap_samples.mean()\n\n\nsns.histplot(bootstrap_sampled_mean, stat='probability')\nplt.title(fr\"Sampling Distribution for $\\bar X_n$, N={len(samples)}\")\nplt.show()\n\n\n\n\nAdd the PDF for the empirical and theoratical distribution\n\nfrom scipy.stats import norm\n\n\nsmean = samples.mean()\nsvar = samples.var() / len(samples)\nsstd = np.sqrt(svar)\n\n\nsns.kdeplot(bootstrap_sampled_mean, label='empirical', color='salmon')\nsns.lineplot(\n    x=(x := np.linspace(min(bootstrap_sampled_mean), max(bootstrap_sampled_mean))), \n    y=norm.pdf(x, loc=smean, scale=sstd),\n    color='skyblue',\n    label='theoratical'\n)\nplt.title(fr\"PDF for $\\bar X_n$, N={len(samples)}\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/hypothesis-testing/hypothesis-testing.html#hypothesis-testing-1",
    "href": "posts/hypothesis-testing/hypothesis-testing.html#hypothesis-testing-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Now suppose we want to know if the mean is \\(p = 0.075\\)\nWe construct the belief, the null hypothesis, \\(H_0: p = 0.075\\)\nGiven the null hypothesis is true we construct the theoratical distribution for sampled mean.\n\ntrue_mean = 0.065\ntrue_se = np.sqrt(true_mean * (1-true_mean) / size)\n\n\nx = np.linspace(true_mean - 4 * true_se, true_mean + 4 * true_se)\ny = norm.pdf(x, loc=true_mean, scale=true_se)\n\nx_fill = np.linspace(true_mean - 4 * true_se, samples.mean())\ny_fill = norm.pdf(x_fill, loc=true_mean, scale=true_se)\n\n\nsns.lineplot(x=x, y=y, label='theoratical')\nplt.axvline(x=samples.mean(), linestyle='-', color='salmon', label='observed mean')\nplt.axvline(x=true_mean, color='lightgray', label='H0 mean')\nplt.fill_between(x_fill, y_fill, color='skyblue', alpha=0.3, label='extreme prob.')\n\nplt.title(r\"$\\bar X_n$ Sampling Dist. | H0\")\nplt.legend()\nplt.show()\n\n\n\n\nThe shaded area is the probability of observing a realized sample mean more extreme than the current one, which is the p-value.\nThe smaller it gets, the more likely the null hypothesis to be false. Then we could set a conventional threshold (e.g. 5%), then use the threshold to reject the H0.\nWe use conventional 5% threshold. For two-sided test, it will be 2.5% for each side.\nNow we could obtain the probability \\(Pr(X &lt;= observed\\_mean | H0)\\)\n\npvalue = norm.cdf(samples.mean(), loc=true_mean, scale=true_se)\n\n\nprint(f\"The p-value is: {pvalue:.4f}\")\n\nif pvalue &lt;= 0.025:\n    print(f\"Result: Reject H0\")\nelse:\n    print(f\"Result: Could not reject H0\")\n\nThe p-value is: 0.0024\nResult: Reject H0\n\n\n\n\n\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom scipy.stats import ttest_1samp\nfrom scipy.stats.distributions import t\n\n\nttest_1samp(samples, popmean=true_mean, alternative='two-sided')\n\nTtest_1sampResult(statistic=-3.4277980568938475, pvalue=0.000633319473855572)\n\n\n\nproportions_ztest(samples.sum(), len(samples), value=true_mean, alternative='two-sided')\n\n(-3.429513242418691, 0.0006046649260488792)"
  },
  {
    "objectID": "posts/first/index.html",
    "href": "posts/first/index.html",
    "title": "Jupyter + Quarto for notebooks",
    "section": "",
    "text": "基本上把過去的一些 jupyter notebooks 利用 quarto 以及 github page 製作成個人的網頁\n\n\n\n\n\n\nNote\n\n\n\nTODO: document steps for blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Hypothesis Testing\n\n\n\n\n\n\n\nstatistics\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nLiang-Cheng Chen\n\n\n\n\n\n\n  \n\n\n\n\nJupyter + Quarto for notebooks\n\n\n\n\n\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nLiang-Cheng Chen\n\n\n\n\n\n\nNo matching items"
  }
]